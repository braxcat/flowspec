# AI Assisted Development Workflow Transcript

## Introduction
Hello. So today I'm going to go through my AI assisted development workflow. So this was actually adopted by a engineering team at a $4 billion unicorn on a vap replacement project I was doing last month. And I didn't actually know anyone was using it until they asked me to present it divisionwide.

So if you've tried AI coding tools for anything serious, you'll have noticed that they don't really deliver out of the box. They hallucinate, lack context, and they produce generic code. So if you're using them for simple stuff like autocomplete, scaffolding, stuff like that, you might be wondering what you're missing when people claim they have like 10 times productivity on this stuff. And don't know about 10x productivity, but the missing piece is really the workflow that you build around them, which is where you teach it your own development processes. And that's what we're going to go through today.

## Background & Context Management
So I must say that I've never personally used claude code without any of these context management systems built in because I carried over knowledge from using cursor then windurf then cursor again. I tried some of the builder platforms like lovable, bolt and vort and those were okay but when claude code came out I realized that their advantage is that they have a development process built in plus domain specific knowledge and I found I realized with claude code what you can do is you can just basically build that yourself and tailor it specifically for your project and that's what I bootstrap at the beginning of every project.

So for more background on this like when I started this project the first thing I did with as with all projects is start building my custom commands and sub aents that I will be using on the project and I committed this to version control and as I was producing code with this I was also committing the documents for each task to version control so people could use it and follow along but I didn't actually know if anyone was using it until like a couple of weeks later in one of the standup sessions they mentioned it yes actually everyone is using it and they wanted me to present it as I said divisionwide. So this video is essentially going to be that presentation.

So I'm hoping this is going to be useful for anyone getting started with AI assisted development tools like cursor and clawed code or if you've tried them and you've been underwhelmed with the results that you've been getting. There's a couple of principles underlying how I get good results from LLMs in general for both voice AI and AI assisted development. and I'm going to explain what those are and specifically how they apply to AI assisted development in this particular video.

## Demo: Pipecat Starter Template Feature
So today as we go through this, what I'm going to do is I'm going to add a feature to a Pipecat starter template I'm planning to open source. We're going to be adding a JSONbased assistant configuration that will configure the assistant behavior and the pipeline configuration. And we're also going to test out Claude's new Opus 4.5 model to do this.

If you're building production Voice AI systems, you're going to have to read a lot of documentation and also a lot of code before you get started. Pipecat, Vappy, Twilio, you'll have to figure out how to accomplish the task and then you'll have to explain it to the assistant so that it does a good job. So what I've done is I've built specialized sub agents that can handle doing the deep research, research the codebase and um they'll come back with recommendations on how to um approach a solution. Now it's obviously works better if you yourself understand exactly how things work and you can point to different parts of documentation and codebase to get it started. So yeah, that's what we're going to do. I'm going to kick off a task and then I'll explain what happened.

## The Planning Phase
Okay, so let's get this party started. It all begins with the plan task custom command.

Okay, so I want to add a configurability layer here to this project. We're going to build a JSON configuration for the assistant and the pipeline setup. So I want there to be two main nodes. One for the assistant behavior such as the system prompt, first message, last message, and the assistant name. And there'll be another node for the technical pipeline configuration which sets up the services contained in that pipeline.

For the Vappy Expert sub agent, I want you to focus on researching Vap's transient assistant configuration implementation. And for the Pipecat expert sub aent, I want you to basically focus on what services are supported by Pipecat because they don't have a built-in JSON configuration for this and that's what we're building on top of the VP um implementation should just be used to inform our strategy and we should be customizing this for Pipecat's idiomatic pipeline configuration methodology.

Um, at a minimum for the ST I want to support Deepgram Graham's Nova 3 model and Flux model, um, Gladius's models and OpenAI's Whisper model. For the LLM, I want to support the OpenAI family and also Google's Gemini 2.5 Flash at a minimum. And for the TTS, I want to support Rhyme, 11 Labs, and Cartisia. I'm also going to need you to create test cases. So example assistant configs I can use to test this at the end. And I for now want to be able to pass these assistant configuration to the application as a command line parameter. We'll just be passing a reference to the file. When it comes to storing API keys, those will be in the environment um file config, the end file, but the rest of the configuration will be assistant specific and that can go in the main configuration we're building.

Okay, so I'll kick this off. You'll notice that I'm being vague on how to implement things, but specific about what context each expert needs. Um, I like to see basically what the AI comes up with and then correct it. And then in doing this correction, I find that I naturally sort of explain what I where I want to get to with this project. And that gives it just a better understanding of the goals of the project. And notice that the the sub aents aren't actually implementation sub aents. They are research sub aents. The idea is they're providing their recommendations to the main orchestrating agent which is then going to do the implementation. And we use the sub aents really to um allow the system to have more context because each sub aent gets its own context and there's no need to burn tokens in the main orchestrating agent because you'll just be provided a summary back from the research agents.

## Reviewing the Plan
Okay, so that took about 10 minutes and let's have a look what it did. The first thing it does is come up with a name for the task. And then the next thing it does is it uses the vap expert sub aent and the pipe cut expert sub aent to figure out how we can solve this problem based on my directions.

So the way the um vap expert works is I've basically given it access to vap's documentation locally um because they have a public GitHub repo with their documentation all there. Um, so I clone that locally and I give it access to that. I also inform it where the online documentation is and also the swagger documentation which is the open API documentation. So it can it is instructed to research all these things and figure out um how the problem that I've specified is solved in vapy.

uh the pipecat expert. Similarly, this has access to the local um pipecat repo which I've cloned. And I just symbolically link all these external dependencies in a directory called underscore refs. And I just explain to the experts where they can access all this and what they are to do with this, which is obviously just figure out how to solve my problem using all this information. And yeah that then these experts provide back detailed summaries to the main orchestrating agent and then the main agent produces a series of documents I can use to review the plan.

### Documentation Generated
So let's go through the files that it's made.

**Implementation Notes**: Firstly we'll start with the implementation notes. So this initially is where actually the full analysis provided by each of the sub agents is shown in full. So you can review it and make sure it comports with your understanding and also you can like research it yourself based on what it's come up with here. And one of the key things about this is that the pipecat sub agent is instructed to identify what the idiomatic way of solving things using pipcat's framework is. And that's something that the plan task command is also instructed to do. And that's important because I'm generally trying to be a chameleon whenever I'm building on top of frameworks. So I'm extending the existing patterns that are used by that framework and only taking liberties once you get to a level of abstraction where that sort of thing makes sense. And that means that anyone coming into the project to work on it doesn't have to learn anything new basically when I'm when they're um working with my code. And in particular with this starter template, of course, I don't want people to having to be learning Pipecat and also how I'm doing things in this.

**The Plan**: The next document then is the plan which contains the overall approach to solving the problem. And this you can review and make sure it's sensible and request any modifications you need if you disagree with anything.

**Decisions File**: Then we have the decisions file and this is where the assistant communicates any decisions that it's made and also has listed any pending decisions that it needs your input on to proceed. So you can review those and like give your decision on there and see if the decisions is made autonomously are sensible and if it needs any direction there too.

**Status File**: And last but not least is a status file which basically communicates the current progress in the task implementation. As you go through implementing a task, there are going to be changes you'll need to make to the first version that Claude puts together. And as you're doing so, the implementation notes get updated with what's changed, any lessons that have been learned along the way. And there is actually a task context tracker assistant which is used to automatically keep these documents updated once Claude has gone through an implementation cycle. So keeping the documents in sync as you're iterating on a task implementation is important both for the current task and also because there can be lessons learned that are worth capturing in a standalone document for future task implementations.

So I've always got a root docs folder that's separate and has separate subfolders in there like pipecat for example to capture any specific lessons I've learned while implementing a particular task and these task specific documents can be used to build that and I think of this as a way of adding project specific memory. So all memory is really are advanced context management systems. Real memory systems have sophisticated storage and retrieval optimized processes for getting the right memories into the context at the right time. So I've just basically built a dollar store version of this for AI assisted development and it works pretty well. And obviously the key is that the expert sub aents are prompted to check the particular folder that contains the learnings for the specific framework or system that they are expert in and deciding whether to use those in the specific task.

## Other Custom Commands

**Root Cause Analysis**: So let's go through some of the other commands I've got here. So root cause analysis this is basically a collaborative debugging procedure. So essentially I will identify an issue that I'm having with the code. The idea is the assistant's going to investigate, come up with a hypothesis on what it thinks the root cause of this issue is. It's going to add debug logging to the project to establish for sure whether or not it's hypothesis is correct or incorrect. And then it's going to start the server and then it hands back to me to reproduce the issue. So I'll interact with the voice AI and reproduce the issue and then I'll hand back to the assistant and it's going to check logs and determine whether or not its hypothesis was correct. If it is, it's going to reset the codebase using git and add a fix. And then it hands back to me to do the testing and I'll check whether or not it's correct. and that's an actual fix and if it is we that you can commit it and if it's not we continue iterating until we find out the real problem.

So root cause analysis varies project to project. I've got a Nex.js JS project where instead of having it running the backend server for example, you'll have it um run the front end and then also give it access to Puppeteer MCP so that it can launch a specific instance I've got installed of Chrome which is Chrome developer edition which is just used for um clawed code when it's doing its investigation. And this lets it actually have a look at the HTML and the CL the um Chrome dev tools to see whether or not it's solved the issue or not basically. So these are just ways of getting creating a feedback loop for this root cause analysis thing. So you have to do less of the analysis essentially.

**Create Pull Request**: So create pull request is another one I used quite a lot. So this doesn't actually create the pull request. What it does is it makes a document in the docs folder for me with all the information I need to create the pull request. So it actually tells you the differences between the branch you're currently on and the one you want to merge into, which it takes as an argument, the branch you want to merge into, that is. And it also tells me what commands to run in GitHub just to remind me um with the GitHub CLI how I can create a pull request with the uh description and suggested header it gives there.

**Create Command**: So then actually to create the custom command I actually have a create command um custom command and what this does is it has access to Claude's documentation which explains how to create a custom command. There's a web page which explains the best practices and then it also has access to the existing custom commands which it should use as examples to understand what sort of structure I create my custom commands in and also when I'm bootstrapping sometimes I just have an examples folder which I'll instruct it to look at for inspiration but I have to say a lot of these I find are portable and in particular the create command and create sub aent custom commands are what I always use to bootstrap any project.

**Create Sub Agent**: So create sub aent works very similar to create command. There's online documentation on anthropic saying the best practices when creating sub aents and then there's the existing sub aents in here which has access to and again as I mentioned sometimes I provide examples when I'm bootstrapping.

**Prompt Engineering Expert**: So I do also in this project have a prompt engineering expert sub agent which I use to help me draft my prompts and this has the accumulated knowledge of like 18 months or so doing this kind of work. um where I basically apply the same principles that I'm trying to communicate in this video with respect to context management and task decomposition to get good results but in this case it's in the context of actual voice agents in real time which is slightly different and actually that's going to be the subject of my next video hopefully where I describe how to split up very complex prompts into sub nodes in order to provide the most specific context to the assistant for the specific stage in a core flow. Um reduce the tokens that are used overall so that you increase the reliability and reduce latency and cost.

## Implementation & Testing
So now for the fun part, I'm actually going to go through the planning documents and get this feature implemented. So far so good. See this is good. It's found that there's different services for flux and for just deepgram standard in pipcat. It's actually identified the best three models also. OpenAI. That's cool. This looks good. Looks good. Yep. This looks pretty much what I had in mind. Okay. So this needs correction. It's not fall back to current behavior. It should just break. Okay. So then implement no decisions. Okay. provided discrimination is correct. Yep. Okay.

So, there's some stuff I need to correct in the decisions document. Okay. So, I'm just going through the plan and pending decisions. So, for the provider specific parameters, I actually want strict schema per provider. So, you'll be making multiple config classes. For the default behavior without the config, I don't want any fallback. I want it to break if there is no config provided because that's what's going to trigger everything to go off. Essentially, the assistant config is necessary. For the config file extension, I only want to allow JSON. We're not supporting YAML. there's going to be one schema configuration supported um validation strictness um strict with clear er error messages but the error messages should only be visible in logs when you're returning errors via the API um those should just be general messages cuz for security we don't want to be revealing um internal operations or anything like that hot reload support no we don't support that just a static config only restart require for changes. Um, and for environment variables overrides, this actually shouldn't be an issue. There should be a clear separation between what appears in the environment and what appears in the JSON config. The environment variable is just for system level config. So, this is going to be where our API keys go and everything else is going to be configured in the assistant config. So, if you could make those changes to the document so I could review them, I would appreciate that. Okay.

Okay, so it's completed that within 5 minutes. Just going to do a second pass over the documents. Make sure everything's okay. Plan looks good. Decisions. So everything's looking good now. So I'm just actually going to get it to kick off the task. Planning documents now look good. Please go ahead and complete the implementation. Ultrathink.

Okay, so it's finished its initial implementation. I was reading the code as it was producing it and I spotted some things I'll need to fix later, but they're relatively minor and I should be able to demonstrate the switching of the TTS at least by changing the assistant config. So, we'll do that for now though. Please add the root configs folder to git ignore because I don't want to be saving the configs to source control and then go ahead and commit the rest of the progress. Okay, so that's done. We'll just do some testing now.

Okay, so switching over to headphones temporarily so you can hear the bot testing without an echo. So I noticed the deep gram nova test is using cartesia and the rhyme test is using flux but it uses rhyme for that. So let's just test to see whether it says the right first message in the right voice. So let's run this v run start with deep RAM. Okay. browser window. Hello, this is a test bot using deep gram Nova. How can I help? Very good. So now let's try with rhyme. Hello, this is a test by using RMTS. How can I help? Great.

## Summary
So, the initial draft is complete. So, I'll obviously need to do more iteration on this to actually get it um sort of production ready. But yeah, that should do for the demonstration.

So then in summary, um in order to get good results with your AI assisted tools or just any AI tools with an LLM component in general, you need to provide the right context context management. So that gives it the best chance at providing a good result. This is basically a system to dynamically build that context per task because dependencies the source code actually often changes. Pipecats often pro releasing new versions. Um, and this just basically keeps it all up to date with current learnings and such. And then there's task decomposition, which is how complex of a task can I give the assistant any at any one time. So this is just the first iteration. We're loading from a file rather than passing it via the API. And it's really just the most basic implementation to get started.

So hopefully that helps. Um, if you found this useful, like, subscribe, comment, all that good stuff. And this template I'm getting ready to release open source. So, you know, follow along if you're interested in that. And also these same principles, context management and task decomposition. I will be making another video on that specifically related to real-time voice interactions. um and basically splitting up very complex pro prompts into multiple nodes and when you should do that and best practices for that.
